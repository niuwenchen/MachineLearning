## 决策树理论
2 计算最优特征子函数
最重要的函数。每种决策树之所以不同，一般都是因为最优特征选择的标准上有差异，不同的标准导致不同类型的决策树。
ID3的最优特征选择标准是信息增益，C4.5是信息增益率，CART是节点方差的大小。

信息熵是事物不确定性的度量标准，也成为信息的单位或“测度”。在决策树中，不仅能用来度量类别的
不确定性，也可以用来度量包含不同特征的数据样本与类别的不确定性。

即某个特征列向量的信息熵越大，说明该向量的不确定成都越大，混乱程度越大，就应优先考虑从该特征向量来进行划分。

详细的说明下公式的过程，不讲公式

ID3: 信息增益

    (1) 计算总数据集的信息熵  ,总数为Num
    (2) 假设取特征A划分，假设特征A有三个值A1,A2,A3，数目分别为Num1,Num2,Num3，计算熵
            假设A1后的子集为S1,那么按照(1)计算子集S1的信息熵，同理计算S2，S3
            计算总的熵，就需要加一个限制: E(A)=Num1/Num*(I(S1))+Num2/Num*(I(S2))+Num3/Num*(I(S3))
    (3) 计算信息增益
            Gain(A) = I(S)-E(A)
            
C4.5: 信息增益率

    虽然信息增益有益于紧凑型的决策树的构建，但也有一个严重的缺陷:对于具有许多输出的校验有很大的偏差
    
    Splitinfo(X)= -Si/S *log(Si/S)
    GainRatio(X)= gain(X)/Splitinfo(X)
    新的增益标准表示分区所生成的有用信息的比例，所谓有用就是有助于分类，
    新的划分方式不再关注于按照某个特征划分后的子集中的样本的类别，而是在关注于前一步，也就是按照某个特征划分后的子集的大小
    
    如: 根据属性1将样本集T划分成3个子集，总的是14，分别是5，4，5 
    info(1) = 5/14 *(I(5)) +4/14*(I(4)) +5/14*(I(5))
    Gain(1)=0.940-info(1)=0.246
    信息增益率则是:
    Splitinfo(1) = -5/14*log2(5/14)-4/14*log2(4/14)-5/14*log2(5/14)  关于于前一步
    GainRatio(1) = Gain(1)/splitinfo(1)=0.246/1.577=0.156
    
    