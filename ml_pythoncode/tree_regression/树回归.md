##树回归
如果数据量变得很大，数据拥有众多特征并且特征之前关系十分复杂，构建全局模型的想法很难实现。

大数据转换为小数据，然后对这部分小数据采用线性回归建模，如果首次切分后仍然难以拟合线性模型就继续切分

CART(Classification and regression tree),ID3对树的划分是采用特征分法，但是这种方法过于迅速，且对连续变量的处理显得不是很好
CART用二元切分法来处理连续型变量，并不是可以处理的程序。一般的伪代码都如下

    找到最佳的切分特征：
        如果该节点不能再分，将该节点存为叶子节点
        执行二元切分
        左右子树分别执行createTree()方法
    
    createTree(dataSet,leafType=modelLeaf,errType=modelErr,ops=(1,10)):
    上面的其他三个参数决定了树的类型:leafType给出建立叶子节点的函数，errType代表误差计算函数
    ops是一个包含构建树所需其他参数的元组。
    
    三个条件返回
        (1): 每一列数据用做二元划分，如果数据全一样，则无法划分，直接作为叶子节点返回
        (2): 总方差，如果总方差-新的误差还在tolS之内，则表示误差的修正效果一般，直接作为叶子节点返回
        (3): 如果有提升，但是根据新的特征的划分，某个子集的数目小于tolN（容忍个数），则直接作为叶子节点返回
    最后返回的是一棵树
    
    后剪枝技术:利用测试集对数据进行剪枝操作，用测试集来测试下这个节点能都合并，如果合并后能降低测试误差则合并，避免数据过拟合。
    
        基于已有的树切分测试数据:
            如果存在任意子集是一棵树，则在该子集递归剪枝过程
            计算当前两个叶节点合并后的误差
            计算不合并的误差
            如果合并会降低误差的话，则将叶子节点合并
           
    {'right': {'right': -0.023838155555555553, 'spVal': 0.197834, 'left': 1.0289583666666666, 'spInd': 1}, 'spVal': 0.39435, 'left': {'right': 1.980035071428571, 'spVal': 0.582002, 'left': {'right': 2.9836209534883724, 'spVal': 0.797583, 'left': 3.9871631999999999, 'spInd': 1}, 'spInd': 1}, 'spInd': 1}
            
                A
            B       C
         D    E F       G
         看一下能否将D和E合并为一个叶节点吗？ D+E/2 就是新的子节点，也就是说假设值 <B, 那么其叶子节点就是小于值B的某一个固定值，而不再
         进行划分。
         
         
     
### 模型树
用数据子集构建线性模型，并计算误差

    模型树的返回不再是数据子集的均值，而是线性模型的ws权重
    

### sklearn 实现 CART




## Tinker

