## 10.1.2 对聚类分析的要求

    可伸缩性:
        聚类算法需要在小数据集上表现良好，也要适应大数据集，需要具有高度可伸缩的聚类算法
    处理不同属性类型的能力:
        数值类型: 基于区间，二元，标称，序数，图，序列和文档各种各样的数据类型
    发现任意形状的簇: 
        许多算法基于欧几里德或曼哈顿距离度量来确定簇，是球形簇，也可能是任意形状的簇
    对于确定输入参数的领域知识的要求
        许多聚类算法都要求用户以输入参数的形式提供领域知识。 因此，聚类结果对这些参数十分敏感。
    处理噪声的能力
        对噪声鲁棒的聚类算法
    增量聚类和输入次序不敏感
        需要开发增量聚类算法和对数据输入次序不敏感的算法
    聚类高维数据的能力：
        发现高维空间中数据对象的簇是一个挑战，特别是考虑这样的数据可能非常稀疏，并且高度倾斜。
    基于约束的聚类：
        找到既满足特定的约束又具有良好聚类特性的数据分组是一项具有挑战性的任务。
    
基本聚类方法:

    划分方法:
        k-均值，k-中心点算法，渐近的提高聚类质量，逼近局部最优解，发现球状簇。
    层次方法：
        层次方法创建给定数据对象集的层次分解。
        凝聚的方法: 自底向上的方法，开始将每个对象作为单独的一个组，逐次合并相近的对象或组，直到所有的组合并为一个组，或者达到终止条件
        分裂的方法: 自顶向下的方法，开始将所有的对象置于一个簇中，在每次相继迭代中，一个簇被划分成更小的簇，直到最终每个对象在单独的一个簇中，或终止条件
        
    基于密度的方法:
        只要邻域中的密度(对象或数据点的数目)超过某个阈值，就增长给定的簇。对给定簇中的每个数据点，在给定半径的邻域中必须至少包含
        最少数目的点。 可以过滤噪声或离群点，发现任意形状的簇。
    
    基于网格的方法:
        将对象空间量化为有限个单元，形成一个网格结构。
    
    
划分方法:

    k-means： 基于质心的方法
        算法复杂度: 对象数n, 簇数k，迭代次数 t, O(nkt)
        
层次方法:
    
    AGENS: 凝聚的层次聚类算法
    DLANA: 分裂的层次聚类算法
    通常使用一种称作树状图的树形结构唉表示层次聚类的过程。
    
    算法距离的度量： 度量的是划分后的两个簇的距离
        最小距离: 最近邻聚类算法，若最小距离超过给定的阈值时聚类过程就会终止，称其为单连接算法
                    使用最小距离度量的层次聚类算法也被称为最小生成树算法
        最大距离: 最远近邻算法，全连接算法。 
        
    BIRCH: 使用聚类特征树的多阶段聚类
        利用层次的平衡迭代规约和聚类(BIRCH)是为大量数值数据聚类设计的，将层次聚类与划分方法集成在一起。
        BIRCH使用聚类特征来概括一个簇，使用聚类特征树(CF-树)来表示聚类的猜呢歌词结构。这些结构帮助聚类方法在大型数据库甚至流数据库中取得好的速度和伸缩性，还使得BIRCH方法对新对象增量或动态聚类也非常有效。
        考虑一个n个d维的数据对象或点的簇。簇的聚类特征(Clustering Feature, CF)是一个3维向量，汇总了对象簇的信息，定义如下：
        CF=<n,LS,SS>
        LS是n个点的线性和， SS是数据点的平方和。
        聚类特征本质上是给定簇的统计汇总。使用聚类特征，可以很容易的推导出簇的许多有用的统计量。
        簇的质心x0, 半径R直径D
        
            x0=LS/n
            R=sqrt((nSS-2LS*LS+nLS)/n*n)
            D=sqrt(2nSS-2LS*LS/(n(n-1)))
        
        R和D都反映了质心周围簇的紧密程度。
        使用聚类特征概括簇可以避免存储个体对象或点的详细信息。只需要固定大小的空间来存放聚类特征。这是空间中BIRCH有效的股那件。　聚类特征是可加的，也就是说，对于两个不相交的簇C1和C2，合并C1和C2后的聚类特征是
        CF1+CF2 = <n1+n2, LS1+LS2,SS1+SS2>
        
        CF树是一颗高度平衡的树，存储了层次聚类的聚类特征。CF树有两个参数：分支因子B和阈值T。分支因子定义了每个
        非叶节点的子女的最大数目，而阈值参数给出了存储在树的叶节点中的子簇的最大直径。这两个参数影响结果树的大小
        
        给定有限的主存，BIRCH一个重要的考虑是最小化IO时间。 BIRCH采用了一种多阶段聚类技术:
            数据集的单遍扫描产生一个基本的好聚类，而一或多遍的额外扫描可以进一步的改进聚类质量。
            阶段1： BIRCH扫描数据库，建立一颗存放于内存的初始CF-树，可以被看作数据的多层压缩，试图保留数据的内在聚类结构。
            阶段2： BIRCH采用某个聚类算法对CF树的叶节点进行聚类，把稀疏的簇当作离群点删除，而把稠密的簇合并为更大的簇。
        
        
        Chameleon: 使用动态建模的多阶段层次聚类
            Chaneleon(变色龙)是一种层次聚类算法，采用动态建模来确定一对簇之间的相似度。 在Chameleon中，
            簇的相似度依据如下两点评估: (1) 簇中对象的连接情况， (2): 簇的邻近性。
            

### 10.4.1 基于密度的方法
簇：数据空间中被稀疏区域分开的稠密区域。基于密度的聚类方法的主要策略，该方法可以发现非球状的簇。

    DBSCAN: 一种基于高密度连通区域的基于密度的聚类
    对象o的密度可以用靠近o的对象数度量。
    DBSCAN(Density-Based Spatial Clustering of Applications with Noise,具有噪声应用的基于密度的空间聚类)找出核心对象，即其领域稠密的对象。连接核心对象和它们的领域，形成稠密区域作为簇。
    “DBSCAN如何确定对象的邻域？” 一个用户指定的半径参数。 对象o的领域以o为中心，用户参数为半径的空间
    DBSCAN使用用户提出的参数MinPts,指定稠密区域的密度阈值。 如果一个对象的邻域至少包含MinPts个对象，则该对象是核心对象。核心对象是稠密区域的支柱。
    给定一个对象集D,可以识别用户参数*和MinPts的所有核心对象， 聚类任务就是归结为使用核心对象和它们的邻域
    形成稠密区域，这里的稠密区域就是簇。 
        对于核心对象q和对象p，说p是从q直接密度可达的，即p在q的*-邻域内。使用密度可达关系，核心对象可以把它的*-邻域中的所有对象都带入一个稠密区域。
        
    “如何使用核心对象为中心的小稠密区域来装配一个大稠密区域？”在DBSCAN中，p是从q密度可达的
    (density-reachable),如果从子啊一个对象链p1,p2,....pn,使得p1=q,pn=1
     
    直接密度可达: 要求From是核心对象，to是对象
    密度可达: 不是等价关系， 并不一定都是核心对象
    密度相连： 如果存在一个对象q属于D,使得p1和p2都是从q关于*和MinPts密度可达的，则对象p1，p2是关于* 
    和Minpts密度相连的。密度相连不要求都是核心对象。密度相连是等价关系。
    
    过程:
        给定初始数据集D中的所有对象都被标记为unvisited，DBSACN随机的选择一个未访问的对象p，标记p为visited，
        并检查p的*-邻域是否至少包含MinPts个对象。如果不是，则p被标记为噪声点。 否则p创建一个新的簇c，并且把
        p的*-邻域中的所有对象都放到集合N中。 DBSCAN迭代的把N中不属于其他簇的对象添加到C中。在此过程中，对于
        N中标记为unvisited的对象p'，DBASCN将其标记为visited，并且检查它的*-邻域。 如果p'的*-邻域至少有
        MinPts个对象，则p'的*-邻域对象都被添加到N中。DBSCAN继续添加对象到C,直到C不能再被拓展，即直到N为空。
        此时，簇C完全生成，于是被输出。
        
            标记所有对象为unvisited
            do
                随机选择一个unvisited对象p
                标记p为visited
                if  p的*-邻域至少含有MinPts个对象
                    创建一个新簇C,并且把p添加到C
                    令N为p的*-邻域中的对象的集合；
                    for  N中的每个点p':
                        if p'是unvisited：
                            标记p'为visited
                            if p'的*-邻域至少有MinPts个点，把这些点添加到N；
                            if p'不是任何簇的成员，把p'添加到C;
                    end for 
                    输出C;
                else 标记p为噪声。
            until 没有标记为unvisited的对象。


### 10.4.2 OPTICS: 通过点排序识别聚类结构
DBSCAN中依赖用户参数e和MinPts，大多数算法都对这些参数值非常敏感：设置的细微不同可能导致差别很大的聚类结果。此外，现实的高维数据集通常具有非常倾斜的分布，全局密度参数不能很好的刻画其内在的聚类结构。

    基于密度的簇关于邻域阈值是单调的。也就是说，在DBSCAN中，对于固定的MinPts和两个邻域阈值e1<e2,关于
    e1和MinPts的簇C一定是关于e2和MinPts的簇C'的子集。 这意味着，如果两个对象在同一个基于密度的簇中，则它们
    一定也在同一个具有较低密度要求的簇中。
为了克服在聚类分析中使用一组全局参数的缺点，提出了OPTICS聚类分析方法。 OPTICS并不显式的产生数据集聚类，而是输出簇排序。这个排序是所有分析对象的线性表，并且代表了数据的基于密度的聚类结构。

    为了同时构造出不同的聚类，对象需要按特定次序处理。这个次序选择这样的对象，即关于最小的e值，它是密度可达的。
    
    OPTICS计算所有对象的排序，并且存储每个对象核心距离和相应的可达距离。OPTICS维护一个称作OrderSeeds的表来产生输出排序。 OrderSeeds中的对象按到各自的核心对象可达距离排序，即按每个对象的最小可达距离排序。
    
### 10.4.3 DENCLUE 基于密度分布函数的聚类
DENCLUE是一种基于一组密度分布函数的聚类算法。

    密度估计: 根据一系列观测数据集来估计不可观测的概率密度函数。在基于密度聚类的背景下，不可观测的概率密度函
    数是待分析的所有可能对象的总体的真实分布。 观测数据集被看作取自该总体的一个随即样本。
    
### 10.6 聚类估计

    1 估计聚类趋势: 霍普金斯统计量
    2 确定簇数: sqrt(n/2) sqrt(2n)
    3 测定聚类质量: 外在方法和内在方法
    
        
        
        
    
        