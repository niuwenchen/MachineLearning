## KMEANS
K均值聚类算法中的K是指聚类中心的个数，k是用户给定的，但是并不一定，还有其他的算法来确定k的个数。

先是计算每一个数据点到质心的距离，寻找最近的质心，将这个点分配到距离其最近的质心。

更新质心。

再次计算每个数据点到质心的距离，如果任意一个数据点分配结果发生改变，重新计算所有点的质心和距离。


一般是这样的 质心为k

{k0:[a,b,c]},{k1:{d,f,f}},但是这样在后面比较距离的时候太麻烦，在demo中实现是用矩阵实现

    mat(np.zeros([k,n])) k行，n列，n列存储的是距离。
    
     [<map object at 0x0000000008832C88>]]:  在初始构造数据的时候，就使用list(map(float,curline))
    
    质心: 二维平面 ，迭代次数也是可以算出来的
    [[-3.17006745  2.60393509]
    [-1.595569    3.01158056]
     [-0.83188333 -2.97988206]
    [ 2.95373358  2.32801413]]
    
还是前面说的，k的问题，K均值聚类收敛但聚类效果较差的原因是K均值算法收敛到了局部最小值，而非全局最小值。

一种用于度量聚类效果的指标是SSE(误差平方和)。误差平方和越小，则数据点越接近于它们的质心，聚类效果也越好。因为对误差取了平方，因此更加重视那些远离中心的点。一种肯定可以降低SSE的方法是增加簇的个数，但是违背了聚类的目标。聚类的目标是在保持
簇数目不变的情况下提高簇的质量。

聚类量化方法： 合并最近的质心，或者合并两个使得SSE增幅最小的质心。第一种思路是通过计算所有质心之间的距离，
然后合并距离最近的两个点实现。第二种方法需要合并两个簇然后计算总SSE值。必须在所有可能的两个簇上重复上述过程，直到找到
合并最佳的两个簇为止。

K均值局部收敛，那么用二分k均值实现。
