## 预测数值型数据 回归

    线性回归
    局部加权线性回归
    岭回归和逐步线性回归
    

### 线性回归
Y1=X1^T*w, 求w， 对其求平方误差。对平方误差展开成矩阵相乘的形式，对其求导， 则ws=(X^(T)X)^(-1) X^(T) y

### 局部加权线性回归
线性回归求的是具有最小均方误差的无偏估计，可能出现欠拟合现象？？？

LWLR， 给待预测点附近的每个点赋予一定的权重

    ws=(XTWX)-1XTWy
    就是给每个点赋予权重，权重采用“核”，高斯核
    
        w(i,i) = exp(|x(i)-x|/-2k^2)
    

### 缩减系数来理解数据

    如果数据的特征比样本点还多怎么办，能不能用之前的方法训练，不能，在(XTX)的时候出错
    GBDT训练过程中数据多
    
    岭回归: 就是在矩阵XTX 上加入一个lambda*I 从而使矩阵非奇异，进而对 XTX+lambda*I求逆
    Ｉ是一个ｍ*m的单位矩阵
    
    通过引入lambda限制了所有w之和，通过引入该惩罚项，能够减少不重要的参数，在统计学中叫缩减，
    GBDT方式
    
    
回归也是预测目标值的一种，回归与分类的不同点在于，前者是预测连续性变量，后者是预测离散型变量，
在回归方程中，求得特征对应的最佳回归系数的方法是最小化误差的平方和，给定输入矩阵，如果XTX的逆存在并可以求得
的话，回归发都可以使用

当数据的样本书比特征数还少的时候，矩阵XTX的逆不能直接计算，即便当样本数比特征数多时，逆仍有可能无法计算，这是因为特征可能高度
相关，考虑使用岭回归，能保证当XTX的逆不能计算时，仍保证能求得回归参数

岭回归是缩减法的一种，相当于对回归系数的大小施加了限制，