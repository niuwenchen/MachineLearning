## 神经网络初步  BP,SOM
BP 反向误差传播神经网络说明：

https://wenku.baidu.com/view/2244b3ffc8d376eeaeaa3176.html

计算过程：
    
    输入，隐含，输出
    隐含层有4个节点，做一个数据的传输过程分析
    假设输入输入行数不知道，列数知道 
    假设2列，那么隐含层如何设计？ 行数已知4，列数和输入的列数一致，因为输入层要加一个偏置，则隐含层在维度上保持一直，也增加一列
    
    在loadDataSet的时候，给输入数据增加了一列偏置，但是实际上的数据维度是2列，因此 nSampleDim = n-1 
    
    307*3的数据转置之后， self.hi_wb * SampleIn = 4*307 
    注意： 隐含层的计算方式: 其实数据的输入是x(x1,x2,...,xn) 这里的括号中的xi 是在列的说明，即一行数据有n列，这n列分别乘以
    第一个隐含层节点的w权重就是隐含层的第一个节点的数据， 再用n乘以第二个隐含层节点对应的w就是第二个节点现在的数据。
    
    4*3 * 3 * 307 = 4* 307  乘法过程已经有求和过程了
    调用逻辑函数 控制隐含层输出: 4*307 列数据
    
    现在的数据是4*307 也就是前面输入层类似的 307 * 3， 现在给输入数据增加一列作为偏置， 计算输出层。
    变成 5*307，也即 1*5列 ，5个输入节点，输出节点只有一个，对应于输出个节点的5个权重
    1*5 * 5*307 = 1*307  307 个输出,调用逻辑函数
    
    现在已经有输出，将输出和样本做均方误差。 如果这个误差小于阈值误差，则结束循环，如果不小于，则开始误差反向传播计算 
    对逻辑函数进行求导，得出的dlogit函数是y(1-y)
    
    输出层梯度: Ferror(do-yo) * F'(yi)  DELTA
    隐含层梯度： multiply(self.out_wb[:,:-1].T*DELTA,self.dlogit(hi_output))  都是按照公式来计算
    更新
    还是按照公式：
    dout_wb = DELTA*hi2out.T        # 输出层权值微分
            dhi_wb = delta*SampleIn.T       #
    执行更新：
    总的就是这样的。
    
    out_wb
    [[-9.6184219  -7.1273985  -3.00220401 -1.5791096   5.53337261]]
    
    hi_wb
    [[ 9.80027636  8.84164372 -8.85235401]
    [-7.74911286 -9.37757926 -6.23705128]
    [-4.51980552 -2.55021578 -4.14875376]
    [-2.48824387 -1.5864871  -4.11443216]]
       
    如何分类？计算出了隐含层和输出层的权重，如何绘制分流线？
    它是构造一批输入数据，-3,3 然后将这批输入数据按照BP神经网络流入的方式进行测试，得出输出结果
    30 *30 = 900 个点，将这900个点进行绘制
    
    隐含层：(4*3 * 3* 3*1 ) hi_wb * x = 4*1 是4个隐含层的值
    
    输出层: 1*5 * （5*1） 所以要给隐含层的输出增加一行，得到最终的输出。
    
    就得到了每一个点对应的分类结果。
    
    画线：
    contour 绘制等值线。什么是等值线
    等值线是制图对象某一数量指标值相等的各点连成的平滑曲线,
    这里将输入的每一个点的相等的值连接起来就形成了分类线，为什么会出现这种将数据分隔的情况呢？
    其实由于数据的分布导致了神经网络权重的值，而根据这些权重就可以计算出每一个点的逻辑回归值。
    
    数据集 --> 神经网络的权重--> 等高线。
    绘制误差曲线之后发现 误差逐渐减小。
    
评估：

BP神经网络存在的一些问题：

网络设计复杂： 在实际应用中，BP神经网络是难以调优的一类网络，网络的设计比较复杂，即便调优，也只适合某类样本集，样本集一旦改变，网络常常需要重新设计，部署成本比较高。

收敛速度慢： 神经网络原生算法的收敛速度都由学习速率(步长)来控制，BP网络也不例外，基本的思路是通过动态修改学习速率来调节，但这一些对于BP网络很困难，或者说对于非线性问题有一定困难，n维曲面在梯度变化上可能没有规律性。调节学习速率的策略是：
在迭代中么次检验权值是否导致误差函数的降低，如果降低，则当前的学习速率可以增加；如果不是，那就应该降低学习速率。

容易陷入局部最优: 在没有附加动量因子之前，网络可能陷入局部最小值，附加动量因子的作用是有可能滑过这些极小值。

## 自组织特征映射神经网络SOM
主要用于解决模式识别类的问题。SOM网络属于无监督学习算法。

    将距离小的个体集合划分为同一类别，将距离大的个体集合划分为不同的类别。
    存在竞争层和输入层
    输出层网络一般是根据数据集的维度来构建输出层网络。例如二维情况,希望分成4类，输出层可设计4*2 的矩阵。
    
    权重根据输入层的维度和输出层的分类数目决定， 行:输入层维度， 列:输出分类数目，一般给０～１　随机值
    
    定义学习率和聚类半径函数，　随着迭代的次数增加而收敛
    
http://blog.csdn.net/wj176623/article/details/52526617

聚类过程

    (1)：　计算学习率和学习半径，随机选择一个样本
    (2): 寻找获胜节点：　计算随机样本与ｗ的点积，找出最小的节点
    (3): 计算优胜领域：　d1= ceil(minIndx/self.M)，　
                       d2= mod(minIndx,self.M)　这两个函数，根据这两个点计算出聚类的领域，
    
    

    
    
    
    
    
    

