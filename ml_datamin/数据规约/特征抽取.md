## feature extraction

### 从dict获取特征
将dict数据转换成python的numpy可以使用的数据， 类似于on-hot-encoding


稀疏

由于大多数文档通常会使用语料库中使用的单词的非常小的子集，因此得到的矩阵将具有许多特征值，这些特征值为零（通常超过其99％）。

例如，10,000个短文本文档（如电子邮件）的集合将使用总共100,000个独特词的大小的词汇，而每个文档将单独使用100到1000个独特的单词。

为了能够将这样的矩阵存储在存储器中，而且加速代数运算矩阵/向量，实现通常将使用诸如scipy.sparse包中可用的实现的稀疏表示 。

字典

    corpus = [
        "This is the first document",
        "Thos is the second second document",
        "And the third one",
        "Is this the first document"
        ]
        

    [[0 1 1 1 0 0 1 0 1 0]
    [0 1 0 1 0 2 1 0 0 1]
    [1 0 0 0 1 0 1 1 0 0]
    [0 1 1 1 0 0 1 0 1 0]]
    
    请注意，在上一个语料库中，第一个和最后一个文档具有完全相同的单词，因此以相等的向量编码。特别是我们失去了最后一个文
    件是一个疑问形式的信息。
    
    bigram_vectorizer = CountVectorizer(ngram_range=(1, 3),
                      token_pattern=r'\b\w+\b', min_df=1)
    
    ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool', 'bi grams are', 'grams are 
    cool']

    [[0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0]
    [0 0 0 1 0 0 1 1 0 1 0 0 0 2 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1]
    [1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0]
    [0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0]]
    
    注重了字典的顺序，但是稀疏型增加。
    
TF-IDF加权


解码文本文件

    文本由字符组成，但文件由字节组成。这些字节表示根据某些编码的字符。要使用Python中的文本文件，它们的字节必须被解码为
    一个称为Unicode的字符集。
    scikit-learn中的文本功能提取器知道如何解码文本文件，但只有当您告诉他们文件的编码
    时，CounterVectorizer才能使用这个encoding参数，大多数是utf-8
    
    
    text1 = b"Sei mir gegr\xc3\xbc\xc3\x9ft mein Sauerkraut"
    text2 = b"holdselig sind deine Ger\xfcche"
    text3 = b"\xff\xfeA\x00u\x00f\x00 \x00F\x00l\x00\xfc\x00g\x00e\x00l\x00n\x00 \x00d\x00e\x00s\x00 \x00G\x00e\x00s\x00a\x00n\x00g\x00e\x00s\x00,\x00 \x00H\x00e\x00r\x00z\x00l\x00i\x00e\x00b\x00c\x00h\x00e\x00n\x00,\x00 \x00t\x00r\x00a\x00g\x00 \x00i\x00c\x00h\x00 \x00d\x00i\x00c\x00h\x00 \x00f\x00o\x00r\x00t\x00"

    print(chardet.detect(text1)["encoding"])
    
应用和实例

    
